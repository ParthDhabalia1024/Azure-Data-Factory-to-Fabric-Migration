import json
import re
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple

import streamlit as st
from azure.identity import InteractiveBrowserCredential
from azure.mgmt.resource import SubscriptionClient
from azure.mgmt.resource import ResourceManagementClient
from azure.mgmt.datafactory import DataFactoryManagementClient
from azure.mgmt.storage import StorageManagementClient
from azure.storage.blob import BlobServiceClient
from azure.storage.filedatalake import DataLakeServiceClient


def _to_dict(obj: Any) -> Dict[str, Any]:
    if obj is None:
        return {}
    if isinstance(obj, dict):
        return obj
    if hasattr(obj, "as_dict"):
        try:
            return obj.as_dict()  # type: ignore[attr-defined]
        except Exception:
            pass
    # Fallback: best-effort JSON round-trip
    try:
        return json.loads(json.dumps(obj))
    except Exception:
        return {}


# Scoring helpers
CONTROL_ACTIVITY_TYPES = {
    "foreach",
    "until",
    "ifcondition",
    "switch",
    "executepipeline",
}

CONNECTIVITY_COMPLEX_KEYWORDS = {
    "onprem",
    "sqlserver",
    "oracle",
    "db2",
    "informix",
    "odbc",
    "sftp",
    "ftp",
    "sap",
    "private",
    "vnet",
}

def _score_component_parity(total_acts: int, non_migratable: int) -> int:
    if total_acts <= 0 or non_migratable <= 0:
        return 0
    ratio = non_migratable / max(total_acts, 1)
    if non_migratable <= 2 and ratio <= 0.1:
        return 1
    if non_migratable <= 5 and ratio <= 0.3:
        return 2
    return 3

def _score_non_migratable(non_migratable: int) -> int:
    if non_migratable <= 0:
        return 0
    if non_migratable <= 2:
        return 1
    if non_migratable <= 5:
        return 2
    return 3

def _score_connectivity(ls_types: Iterable[str]) -> int:
    types = [t.lower() for t in ls_types if t]
    if not types:
        return 0
    flagged = sum(1 for t in types if any(k in t for k in CONNECTIVITY_COMPLEX_KEYWORDS))
    if flagged == 0:
        return 0
    if flagged <= 1:
        return 1
    if flagged <= 3:
        return 2
    return 3

def _score_orchestration(total_acts: int, control_acts: int) -> int:
    if total_acts <= 5 and control_acts == 0:
        return 0
    if total_acts <= 10 and control_acts <= 1:
        return 1
    if total_acts <= 20 or control_acts <= 3:
        return 2
    return 3

def _extract_dataset_references(activity: Dict[str, Any]) -> Set[str]:
    refs: Set[str] = set()
    for key in ("inputs",):
        items = activity.get(key)
        if isinstance(items, list):
            for item in items:
                if isinstance(item, dict):
                    ref = item.get("referenceName") or item.get("name")
                    if isinstance(ref, str) and ref:
                        refs.add(ref)
    return refs

def _extract_linked_service_reference(dataset: Dict[str, Any]) -> str:
    props = dataset.get("properties") if isinstance(dataset, dict) else None
    if not isinstance(props, dict):
        return ""
    lsn = props.get("linkedServiceName")
    if isinstance(lsn, dict):
        rn = lsn.get("referenceName")
        if isinstance(rn, str):
            return rn
    if isinstance(lsn, str):
        return lsn
    return ""
    if isinstance(obj, dict):
        return obj
    if hasattr(obj, "as_dict"):
        return obj.as_dict()
    return json.loads(json.dumps(obj))


def _collect_activity_types(activities: Optional[List[Any]], types: Set[str]) -> None:
    if not activities:
        return
    for act in activities:
        a = _to_dict(act)
        t = a.get("type")
        if t:
            types.add(t)
        inner_lists: List[List[Any]] = []
        for key in (
            "activities",
            "ifTrueActivities",
            "ifFalseActivities",
            "defaultActivities",
            "innerActivities",
            "caseActivities",
        ):
            v = a.get(key)
            if isinstance(v, list):
                inner_lists.append(v)
        cases = a.get("cases")
        if isinstance(cases, list):
            for c in cases:
                cd = _to_dict(c)
                if isinstance(cd.get("activities"), list):
                    inner_lists.append(cd.get("activities"))
        for lst in inner_lists:
            _collect_activity_types(lst, types)


@st.cache_data(show_spinner=False)
def list_subscriptions(_credential: InteractiveBrowserCredential) -> List[Tuple[str, str]]:
    client = SubscriptionClient(_credential)
    subs = list(client.subscriptions.list())
    return [(s.display_name or s.subscription_id, s.subscription_id) for s in subs]


@st.cache_data(show_spinner=False)
def list_resource_groups(_credential: InteractiveBrowserCredential, subscription_id: str) -> List[str]:
    rg_client = ResourceManagementClient(_credential, subscription_id)
    return [rg.name for rg in rg_client.resource_groups.list()]


@st.cache_data(show_spinner=False)
def list_data_factories(_credential: InteractiveBrowserCredential, subscription_id: str, resource_group: str) -> List[str]:
    adf_client = DataFactoryManagementClient(_credential, subscription_id)
    return [f.name for f in adf_client.factories.list_by_resource_group(resource_group)]


@st.cache_data(show_spinner=False)
def list_rg_resources(_credential: InteractiveBrowserCredential, subscription_id: str, resource_group: str) -> List[Dict[str, str]]:
    rg_client = ResourceManagementClient(_credential, subscription_id)
    rows: List[Dict[str, str]] = []
    for res in rg_client.resources.list_by_resource_group(resource_group):
        d = _to_dict(res)
        full_type = d.get("type") or getattr(res, "type", "")
        rows.append({
            "Type": _friendly_resource_type(full_type),
            "Name": d.get("name") or getattr(res, "name", ""),
        })
    return rows


@st.cache_data(show_spinner=False)
def list_storage_accounts(
    _credential: InteractiveBrowserCredential,
    subscription_id: str,
    resource_group: str,
) -> List[str]:
    smc = StorageManagementClient(_credential, subscription_id)
    return [sa.name for sa in smc.storage_accounts.list_by_resource_group(resource_group)]


@st.cache_data(show_spinner=False)
def list_blob_containers(
    _credential: InteractiveBrowserCredential,
    subscription_id: str,
    resource_group: str,
    account_name: str,
) -> List[str]:
    # Prefer data plane (RBAC) if possible
    try:
        svc = BlobServiceClient(account_url=f"https://{account_name}.blob.core.windows.net", credential=_credential)
        return [c.name for c in svc.list_containers()]
    except Exception:
        pass
    # Fallback to management plane
    try:
        smc = StorageManagementClient(_credential, subscription_id)
        return [c.name for c in smc.blob_containers.list(resource_group, account_name)]
    except Exception as exc:
        raise exc


def _blob_service(
    _credential: InteractiveBrowserCredential,
    account_name: str,
) -> BlobServiceClient:
    return BlobServiceClient(account_url=f"https://{account_name}.blob.core.windows.net", credential=_credential)


def is_hns_enabled(
    _credential: InteractiveBrowserCredential,
    subscription_id: str,
    resource_group: str,
    account_name: str,
) -> bool:
    try:
        smc = StorageManagementClient(_credential, subscription_id)
        props = smc.storage_accounts.get_properties(resource_group, account_name)
        d = _to_dict(props)
        # Common property names across SDKs
        return bool(
            d.get("is_hns_enabled")
            or d.get("is_hns")
            or getattr(props, "is_hns_enabled", False)
            or getattr(props, "is_hns", False)
        )
    except Exception:
        return False


def _dfs_service(
    _credential: InteractiveBrowserCredential,
    account_name: str,
) -> DataLakeServiceClient:
    return DataLakeServiceClient(account_url=f"https://{account_name}.dfs.core.windows.net", credential=_credential)


def _path_info(path: Any) -> Dict[str, Any]:
    info: Dict[str, Any] = {}
    name = getattr(path, "name", None)
    if callable(name):
        name = name()
    info["name"] = name

    is_dir = getattr(path, "is_directory", None)
    if callable(is_dir):
        is_dir = is_dir()
    if isinstance(is_dir, str):
        is_dir = is_dir.lower() == "true"
    info["is_directory"] = bool(is_dir)

    content_length = getattr(path, "content_length", None)
    if callable(content_length):
        content_length = content_length()
    info["content_length"] = content_length

    last_modified = getattr(path, "last_modified", None)
    if callable(last_modified):
        last_modified = last_modified()
    if last_modified is not None:
        info["last_modified"] = str(last_modified)
    else:
        info["last_modified"] = None

    return info


def list_adls_top_level_directories(
    _credential: InteractiveBrowserCredential,
    account_name: str,
    filesystem: str,
) -> List[Dict[str, str]]:
    try:
        svc = _dfs_service(_credential, account_name)
        fs = svc.get_file_system_client(filesystem)
        top_levels: Dict[str, Optional[str]] = {}
        # Use recursive=True to discover first segments even when only nested dirs exist
        for p in fs.get_paths(path="", recursive=True):
            info = _path_info(p)
            name = info.get("name") or ""
            if not name:
                continue
            is_dir = info.get("is_directory", False)
            # For directories, take their first segment; for files, skip
            if is_dir and "/" in name:
                folder = name.split("/", 1)[0]
                top_levels.setdefault(folder, info.get("last_modified"))
            elif is_dir and "/" not in name:
                top_levels.setdefault(name, info.get("last_modified"))
        return [
            {
                "Folder": folder,
                "LastModified": top_levels[folder] or "",
            }
            for folder in sorted(top_levels)
        ]
    except Exception as exc:
        raise exc


def list_adls_files_in_directory(
    _credential: InteractiveBrowserCredential,
    account_name: str,
    filesystem: str,
    directory: str,
    max_items: int = 500,
) -> List[str]:
    try:
        svc = _dfs_service(_credential, account_name)
        fs = svc.get_file_system_client(filesystem)
        files: List[str] = []
        for p in fs.get_paths(path=directory, recursive=False):
            info = _path_info(p)
            is_dir = info.get("is_directory", False)
            if not is_dir:
                name = info.get("name") or ""
                if name:
                    rel = name
                    prefix = f"{directory.rstrip('/')}/"
                    if directory and name.startswith(prefix):
                        rel = name[len(prefix):]
                    files.append(rel)
            if len(files) >= max_items:
                break
        return files
    except Exception as exc:
        raise exc


def list_top_level_folders(
    _credential: InteractiveBrowserCredential,
    account_name: str,
    container_name: str,
) -> List[str]:
    try:
        svc = _blob_service(_credential, account_name)
        cc = svc.get_container_client(container_name)
        folders: Set[str] = set()
        for blob in cc.list_blobs():
            name = getattr(blob, "name", "") or _to_dict(blob).get("name", "")
            if "/" in name:
                folders.add(name.split("/", 1)[0])
        return sorted(folders)
    except Exception as exc:
        raise exc


def list_files_in_folder(
    _credential: InteractiveBrowserCredential,
    account_name: str,
    container_name: str,
    folder: str,
    max_items: int = 200,
) -> List[str]:
    try:
        svc = _blob_service(_credential, account_name)
        cc = svc.get_container_client(container_name)
        prefix = folder.rstrip("/") + "/"
        files: List[str] = []
        for blob in cc.list_blobs(name_starts_with=prefix):
            name = getattr(blob, "name", "")
            if name and not name.endswith("/"):
                files.append(name)
            if len(files) >= max_items:
                break
        return files
    except Exception as exc:
        raise exc


def sample_adls_paths(
    _credential: InteractiveBrowserCredential,
    account_name: str,
    filesystem: str,
    limit: int = 20,
) -> List[Dict[str, Any]]:
    try:
        svc = _dfs_service(_credential, account_name)
        fs = svc.get_file_system_client(filesystem)
        samples: List[Dict[str, Any]] = []
        for idx, p in enumerate(fs.get_paths(path="", recursive=True)):
            info = _path_info(p)
            samples.append(info)
            if idx + 1 >= limit:
                break
        return samples
    except Exception as exc:
        raise exc


def sample_blob_paths(
    _credential: InteractiveBrowserCredential,
    account_name: str,
    container_name: str,
    limit: int = 20,
) -> List[Dict[str, Any]]:
    try:
        svc = _blob_service(_credential, account_name)
        cc = svc.get_container_client(container_name)
        samples: List[Dict[str, Any]] = []
        for idx, blob in enumerate(cc.list_blobs()):
            bd = _to_dict(blob)
            samples.append({
                "name": bd.get("name") or getattr(blob, "name", ""),
                "size": bd.get("size") or getattr(blob, "size", None),
                "content_type": bd.get("content_settings", {}).get("content_type") if isinstance(bd.get("content_settings"), dict) else None,
                "last_modified": str(bd.get("last_modified") or getattr(blob, "last_modified", "")),
            })
            if idx + 1 >= limit:
                break
        return samples
    except Exception as exc:
        raise exc


def _friendly_resource_type(full_type: Optional[str]) -> str:
    if not full_type:
        return "Unknown"
    normalized = full_type.strip()
    provider, _, resource_path = normalized.partition("/")
    provider_tokens = _clean_provider(provider)
    if provider_tokens and provider_tokens[0].lower() == "microsoft":
        provider_tokens = provider_tokens[1:]
    resource_token = _clean_resource_segment(resource_path)

    provider_name = " ".join(provider_tokens).strip()

    if resource_token and resource_token.lower() in provider_name.lower():
        return provider_name or resource_token
    if provider_name and resource_token:
        combined = f"{provider_name} {resource_token}".strip()
        return _dedupe_words(combined)
    return provider_name or resource_token or normalized


def _split_camel(value: str) -> str:
    return re.sub(r"(?<=[a-z0-9])(?=[A-Z])", " ", value)


def _clean_provider(provider: str) -> List[str]:
    if not provider:
        return []
    parts = []
    for token in provider.split('.'):
        token = token.strip()
        if not token:
            continue
        cleaned = _split_camel(token.replace('_', ' ').replace('-', ' '))
        parts.extend(t for t in cleaned.split() if t)
    return parts


def _clean_resource_segment(resource_path: str) -> str:
    if not resource_path:
        return ""
    last_segment = resource_path.split('/')[-1]
    cleaned = _split_camel(last_segment.replace('_', ' ').replace('-', ' ')).strip()
    if cleaned.lower().endswith('ies'):
        cleaned = cleaned[:-3] + 'y'
    elif cleaned.lower().endswith('s') and len(cleaned) > 3:
        cleaned = cleaned[:-1]
    return cleaned.title()


def _dedupe_words(text: str) -> str:
    seen = []
    lower_seen = set()
    for word in text.split():
        lw = word.lower()
        if lw not in lower_seen:
            seen.append(word)
            lower_seen.add(lw)
    return " ".join(seen)


def fetch_components_for_factory(
    
    credential: InteractiveBrowserCredential,
    subscription_id: str,
    resource_group: str,
    factory_name: str,
) -> List[str]:
    adf_client = DataFactoryManagementClient(credential, subscription_id)
    types: Set[str] = set()
    for p in adf_client.pipelines.list_by_factory(resource_group, factory_name):
        name = getattr(p, "name", None) or _to_dict(p).get("name")
        if not name:
            continue
        full = adf_client.pipelines.get(resource_group, factory_name, name)
        fd = _to_dict(full)
        acts = fd.get("activities") or fd.get("properties", {}).get("activities")
        if isinstance(acts, list):
            _collect_activity_types(acts, types)
    return sorted(types)


def _normalize_type(t: Optional[str]) -> str:
    if not t:
        return ""
    s = t.strip().lower()
    if s.endswith("activity"):
        s = s[:-8]
    return s


SUPPORTED_MIGRATABLE = {
    "copy",
    "executepipeline",
    "ifcondition",
    "wait",
    "web",
    "setvariable",
    "azurefunction",
    "foreach",
    "lookup",
    "switch",
    "sqlserverstoredprocedure",
}


def is_migratable(activity_type: Optional[str]) -> bool:
    return _normalize_type(activity_type) in SUPPORTED_MIGRATABLE


def get_activity_category(activity_type: Optional[str]) -> str:
    norm = _normalize_type(activity_type)
    if not norm:
        return "Other"

    direct_map = {
        "copy": "Move & Transform",
        "executepipeline": "Orchestration",
        "ifcondition": "Orchestration",
        "wait": "Orchestration",
        "web": "External Service",
        "setvariable": "Orchestration",
        "azurefunction": "Compute",
        "foreach": "Orchestration",
        "lookup": "Data Lookup",
        "switch": "Orchestration",
        "sqlserverstoredprocedure": "Database",
        "notebook": "Synapse Notebook",
        "executedataflow":"General"
    }

    if norm in direct_map:
        return direct_map[norm]

    if "databricks" in norm and "notebook" in norm:
        return "Databricks Notebook"
    if "synapse" in norm and "notebook" in norm:
        return "Synapse Notebook"
    if "notebook" in norm:
        return "Notebook"
    if "copy" in norm:
        return "Move & Transform"

    return "Other"


def _activity_activation_status(activity: Dict[str, Any]) -> str:
    if not activity:
        return "Unknown"
    disabled = activity.get("isDisabled")
    if isinstance(disabled, bool):
        return "No" if disabled else "Yes"
    state = activity.get("state") or activity.get("status")
    if isinstance(state, str):
        state_lower = state.lower()
        if state_lower in {"disabled", "inactive", "off"}:
            return "No"
        if state_lower in {"enabled", "active", "on"}:
            return "Yes"
    return "Yes"


def _unwrap_expr(val: Any) -> Optional[str]:
    if isinstance(val, str) and val.strip():
        return val.strip()
    if isinstance(val, dict):
        for k in ("value", "expression"):
            iv = val.get(k)
            if isinstance(iv, str) and iv.strip():
                return iv.strip()
    return None


def _norm_key(key: Any) -> str:
    try:
        s = str(key)
    except Exception:
        return ""
    s = s.lower()
    # remove non-letters to match variants like sql_reader_query, sqlReaderQuery, sql-reader-query
    import re as _re
    return _re.sub(r"[^a-z]", "", s)


def _extract_sql_query_from_dataset(ds_dict: Dict[str, Any]) -> str:
    if not isinstance(ds_dict, dict):
        return ""
    props = (ds_dict.get("properties") or {})
    tprops = (props.get("typeProperties") or {})
    for k in ("query", "sqlReaderQuery", "commandText"):
        got = _unwrap_expr(tprops.get(k))
        if got:
            return got
    # Case-insensitive / snake-case variants
    lower = {k.lower(): v for k, v in tprops.items()}
    for k in ("query", "sqlreaderquery", "commandtext"):
        got = _unwrap_expr(lower.get(k))
        if got:
            return got

    # Deep search fallback
    def deep_find(o: Any) -> Optional[str]:
        if isinstance(o, dict):
            # direct hits
            for key, val in o.items():
                lk = _norm_key(key)
                if lk in ("query", "sqlreaderquery", "commandtext"):
                    un = _unwrap_expr(val)
                    if un:
                        return un
            # recurse
            for val in o.values():
                res = deep_find(val)
                if res:
                    return res
        elif isinstance(o, list):
            for it in o:
                res = deep_find(it)
                if res:
                    return res
        return None
    found = deep_find(tprops)
    if found:
        return found
    return ""


def _extract_sql_query_from_activity(activity: Dict[str, Any], ds_map: Optional[Dict[str, Dict[str, Any]]] = None) -> str:
    if not isinstance(activity, dict):
        return ""
    # Some SDK shapes put properties under activity["typeProperties"], others under activity["properties"]["typeProperties"]
    # Try direct camelCase
    tp = activity.get("typeProperties")
    # Try nested under properties
    if not isinstance(tp, dict):
        props = activity.get("properties")
        if isinstance(props, dict):
            tp = props.get("typeProperties")
    # Try normalized keys (snake/case-insensitive)
    if not isinstance(tp, dict):
        norm_map = { _norm_key(k): v for k, v in activity.items() }
        tp = norm_map.get("typeproperties")
    if not isinstance(tp, dict):
        props = activity.get("properties")
        if isinstance(props, dict):
            norm_props = { _norm_key(k): v for k, v in props.items() }
            tp = norm_props.get("typeproperties")
    if not isinstance(tp, dict):
        tp = {}
    src = tp.get("source") or {}
    if isinstance(src, dict):
        # Prefer explicit sqlReaderQuery.value for SQL sources
        if "sqlReaderQuery" in src:
            val = _unwrap_expr(src.get("sqlReaderQuery"))
            if val:
                return val
        # Other common fields across sources
        for k in ("query", "commandText"):
            got = _unwrap_expr(src.get(k))
            if got:
                return got

        # Case-insensitive keys
        # Try normalized-key lookup as well
        by_norm: Dict[str, Any] = {}
        for k, v in src.items():
            by_norm[_norm_key(k)] = v
        for k in ("sqlreaderquery", "query", "commandtext"):
            got = _unwrap_expr(by_norm.get(k))
            if got:
                return got

        # Deep search within source
        def deep_find(o: Any) -> Optional[str]:
            if isinstance(o, dict):
                for key, val in o.items():
                    lk = _norm_key(key)
                    if lk in ("query", "sqlreaderquery", "commandtext"):
                        un = _unwrap_expr(val)
                        if un:
                            return un
                for val in o.values():
                    res = deep_find(val)
                    if res:
                        return res
            elif isinstance(o, list):
                for it in o:
                    res = deep_find(it)
                    if res:
                        return res
            return None
        found = deep_find(src)
        if found:
            return found

    # Fall back to dataset-level query if inputs reference a dataset with a query
    if isinstance(ds_map, dict):
        inputs = activity.get("inputs")
        if isinstance(inputs, list):
            for ref in inputs:
                if not isinstance(ref, dict):
                    continue
                rn = ref.get("referenceName") or ref.get("name")
                if isinstance(rn, str) and rn:
                    ds_def = ds_map.get(rn)
                    if ds_def:
                        q = _extract_sql_query_from_dataset(ds_def)
                        if q:
                            return q

    # Ultimate fallback: deep search entire activity for sqlReaderQuery/query/commandText
    def deep_find(o: Any) -> Optional[str]:
        if isinstance(o, dict):
            for key, val in o.items():
                lk = _norm_key(key)
                if lk in ("sqlreaderquery", "query", "commandtext"):
                    un = _unwrap_expr(val)
                    if un:
                        return un
            for val in o.values():
                res = deep_find(val)
                if res:
                    return res
        elif isinstance(o, list):
            for it in o:
                res = deep_find(it)
                if res:
                    return res
        return None
    found_any = deep_find(activity)
    if found_any:
        return found_any

    return ""


def _collect_activity_rows(
    activities: Optional[List[Any]],
    rows: List[Dict[str, str]],
    factory_name: str,
    pipeline_name: str,
    ds_map: Optional[Dict[str, Dict[str, Any]]] = None,
) -> None:
    if not activities:
        return
    for act in activities:
        a = _to_dict(act)
        t = a.get("type")
        name = a.get("name")
        desc = a.get("description") or ""
        activated = _activity_activation_status(a)
        query = _extract_sql_query_from_activity(a, ds_map=ds_map)
        rows.append({
            "Factory": factory_name,
            "PipelineName": pipeline_name,
            "ActivityName": name or "",
            "ActivityType": t or "",
            "Migratable": "Yes" if is_migratable(t) else "No",
            "Category": get_activity_category(t),
            "Activated": activated,
            "Description": desc,
            "SourceQuery": query,
        })

        inner_lists: List[List[Any]] = []
        for key in (
            "activities",
            "ifTrueActivities",
            "ifFalseActivities",
            "defaultActivities",
            "innerActivities",
            "caseActivities",
        ):
            v = a.get(key)
            if isinstance(v, list):
                inner_lists.append(v)
        cases = a.get("cases")
        if isinstance(cases, list):
            for c in cases:
                cd = _to_dict(c)
                if isinstance(cd.get("activities"), list):
                    inner_lists.append(cd.get("activities"))
        for lst in inner_lists:
            _collect_activity_rows(lst, rows, factory_name, pipeline_name, ds_map)


def fetch_activity_rows_for_factory(
    credential: InteractiveBrowserCredential,
    subscription_id: str,
    resource_group: str,
    factory_name: str,
) -> List[Dict[str, str]]:
    adf_client = DataFactoryManagementClient(credential, subscription_id)
    # Build dataset map for dataset-level query resolution
    ds_map: Dict[str, Dict[str, Any]] = {}
    try:
        for ds in adf_client.datasets.list_by_factory(resource_group, factory_name):
            name = getattr(ds, "name", None) or _to_dict(ds).get("name")
            if not name:
                continue
            try:
                full = adf_client.datasets.get(resource_group, factory_name, name)
                ds_map[name] = _to_dict(full)
            except Exception:
                ds_map[name] = _to_dict(ds)
    except Exception:
        pass
    rows: List[Dict[str, str]] = []
    for p in adf_client.pipelines.list_by_factory(resource_group, factory_name):
        name = getattr(p, "name", None) or _to_dict(p).get("name")
        if not name:
            continue
        full = adf_client.pipelines.get(resource_group, factory_name, name)
        fd = _to_dict(full)
        acts = fd.get("activities") or fd.get("properties", {}).get("activities")
        if isinstance(acts, list):
            _collect_activity_rows(acts, rows, factory_name, name, ds_map)
    return rows


def list_linked_services_for_factory(
    credential: InteractiveBrowserCredential,
    subscription_id: str,
    resource_group: str,
    factory_name: str,
) -> List[Dict[str, str]]:
    adf_client = DataFactoryManagementClient(credential, subscription_id)
    items: List[Dict[str, str]] = []
    for ls in adf_client.linked_services.list_by_factory(resource_group, factory_name):
        d = _to_dict(ls)
        ls_type = (
            (d.get("properties") or {}).get("type")
            or d.get("type")
            or getattr(ls, "type", "")
        )
        ls_name = d.get("name") or getattr(ls, "name", "")
        items.append({
            "Factory": factory_name,
            "LinkedService": ls_name,
            "LinkedServiceType": ls_type or "",
        })
    return items


def list_datasets_for_factory(
    credential: InteractiveBrowserCredential,
    subscription_id: str,
    resource_group: str,
    factory_name: str,
) -> List[Dict[str, str]]:
    """Fetch all datasets and their paths/queries for a given ADF factory."""
    adf_client = DataFactoryManagementClient(credential, subscription_id)
    items: List[Dict[str, str]] = []
    for ds in adf_client.datasets.list_by_factory(resource_group, factory_name):
        dd = _to_dict(ds)
        ds_name = dd.get("name") or getattr(ds, "name", None)
        if not ds_name:
            continue

        # Get full definition for type properties
        try:
            full = adf_client.datasets.get(resource_group, factory_name, ds_name)
            dd = _to_dict(full)
        except Exception:
            pass

        props = dd.get("properties") or {}
        dtype = props.get("type") or dd.get("type") or ""

        items.append({
            "Factory": factory_name,
            "DatasetName": ds_name,
            "DatasetType": dtype,
        })
    return items


def _dot_id(prefix: str, name: str) -> str:
    safe = re.sub(r"[^A-Za-z0-9_]+", "_", name or "")
    return f"{prefix}_{safe}"


def _dot_label(text: str) -> str:
    return (text or "").replace("\"", "'")


def build_lineage_dot(
    credential: InteractiveBrowserCredential,
    subscription_id: str,
    resource_group: str,
    factories: List[str],
) -> str:
    adf_client = DataFactoryManagementClient(credential, subscription_id)
    lines: List[str] = []
    lines.append("digraph G {")
    lines.append("  rankdir=LR;")
    lines.append("  node [fontsize=10];")

    added_nodes: Set[str] = set()
    added_edges: Set[Tuple[str, str]] = set()

    def add_node(node_id: str, label: str, shape: str, color: str = "#4c7bd9") -> None:
        if node_id in added_nodes:
            return
        added_nodes.add(node_id)
        lines.append(f'  "{node_id}" [label="{_dot_label(label)}", shape={shape}, style=filled, fillcolor="{color}"];')

    def add_edge(a: str, b: str) -> None:
        key = (a, b)
        if key in added_edges:
            return
        added_edges.add(key)
        lines.append(f'  "{a}" -> "{b}";')

    for fac in factories:
        fac_id = _dot_id("factory", fac)
        add_node(fac_id, f"Factory: {fac}", shape="folder", color="#e6f0ff")

        # Pipelines and activities
        try:
            for p in adf_client.pipelines.list_by_factory(resource_group, fac):
                p_name = getattr(p, "name", None) or _to_dict(p).get("name", "")
                if not p_name:
                    continue
                p_id = _dot_id("pipeline", f"{fac}_{p_name}")
                add_node(p_id, f"Pipeline: {p_name}", shape="box", color="#dff4ea")
                add_edge(fac_id, p_id)

                try:
                    full = adf_client.pipelines.get(resource_group, fac, p_name)
                    fd = _to_dict(full)
                    acts = fd.get("activities") or fd.get("properties", {}).get("activities")
                    if isinstance(acts, list):
                        for act in acts:
                            ad = _to_dict(act)
                            a_name = ad.get("name", "")
                            a_type = ad.get("type", "")
                            if not a_name:
                                continue
                            a_id = _dot_id("activity", f"{fac}_{p_name}_{a_name}")
                            add_node(a_id, f"Activity: {a_name}\n({a_type})", shape="ellipse", color="#fff4e6")
                            add_edge(p_id, a_id)
                except Exception:
                    pass
        except Exception:
            pass

        # Linked services
        ls_rows: List[Dict[str, str]] = []
        try:
            ls_rows = list_linked_services_for_factory(credential, subscription_id, resource_group, fac)
        except Exception:
            ls_rows = []
        ls_type_map: Dict[str, str] = {r.get("LinkedService", ""): r.get("LinkedServiceType", "") for r in ls_rows}
        for r in ls_rows:
            ls_name = r.get("LinkedService", "")
            ls_type = r.get("LinkedServiceType", "")
            if not ls_name:
                continue
            ls_id = _dot_id("ls", f"{fac}_{ls_name}")
            add_node(ls_id, f"LinkedService: {ls_name}\n({ls_type})", shape="cylinder", color="#f0e6ff")
            add_edge(fac_id, ls_id)

        # Datasets (and try to connect to linked service)
        try:
            for ds in adf_client.datasets.list_by_factory(resource_group, fac):
                dn = getattr(ds, "name", None) or _to_dict(ds).get("name", "")
                if not dn:
                    continue
                dtype = ""
                lsn = ""
                try:
                    full = adf_client.datasets.get(resource_group, fac, dn)
                    dd = _to_dict(full)
                    props = (dd.get("properties") or {})
                    dtype = props.get("type") or dd.get("type") or ""
                    lsn_field = props.get("linkedServiceName", {})
                    if isinstance(lsn_field, dict):
                        lsn = lsn_field.get("referenceName", "")
                    elif isinstance(lsn_field, str):
                        lsn = lsn_field
                except Exception:
                    pass

                ds_id = _dot_id("ds", f"{fac}_{dn}")
                add_node(ds_id, f"Dataset: {dn}\n({dtype})", shape="component", color="#e8f5ff")
                if lsn:
                    ls_id = _dot_id("ls", f"{fac}_{lsn}")
                    # Ensure LS node exists with type label if we have it
                    ls_type = ls_type_map.get(lsn, "")
                    add_node(ls_id, f"LinkedService: {lsn}\n({ls_type})", shape="cylinder", color="#f0e6ff")
                    # ADF -> LS is added earlier; now link LS -> Dataset
                    add_edge(ls_id, ds_id)
                else:
                    # No LS reference; fall back to ADF -> Dataset
                    add_edge(fac_id, ds_id)
        except Exception:
            pass

    lines.append("}")
    return "\n".join(lines)


def main() -> None:
    st.set_page_config(page_title="ADF Components Browser", page_icon="ðŸ§©", layout="centered")
    st.title("Azure Data Factory Components Browser")
    st.caption("Sign in, pick subscription â†’ resource group â†’ factories, then view components per factory.")

    if "credential" not in st.session_state:
        st.session_state.credential = None

    # Sign-in section
    with st.container(border=True):
        st.subheader("1. Sign in to Azure")
        col1, col2 = st.columns([1, 3])
        with col1:
            login_clicked = st.button("Sign in", type="primary", use_container_width=True)
        with col2:
            st.info("Interactive browser login. You may be prompted in a separate window.")
        if login_clicked or st.session_state.credential is None:
            try:
                cred = InteractiveBrowserCredential()
                # Touch the graph by listing subscriptions to complete device login
                subs = list_subscriptions(_credential=cred)
                st.session_state.credential = cred
                st.success("Signed in successfully.")
            except Exception as e:
                st.error(f"Sign-in failed: {e}")
                return

    credential: Optional[InteractiveBrowserCredential] = st.session_state.credential
    if credential is None:
        st.stop()

    # Subscription selection
    with st.container(border=True):
        st.subheader("2. Select subscription")
        try:
            subs = list_subscriptions(_credential=credential)
        except Exception as e:
            st.error(f"Failed to list subscriptions: {e}")
            st.stop()
        sub_labels = [f"{name} ({sid})" for name, sid in subs]
        sub_idx = st.selectbox("Subscription", options=list(range(len(subs))), format_func=lambda i: sub_labels[i] if subs else "", index=0 if subs else None)
        if subs:
            subscription_id = subs[sub_idx][1]
        else:
            st.warning("No subscriptions available.")
            st.stop()

    # Resource group selection
    with st.container(border=True):
        st.subheader("3. Select resource group")
        try:
            rgs = list_resource_groups(_credential=credential, subscription_id=subscription_id)
        except Exception as e:
            st.error(f"Failed to list resource groups: {e}")
            st.stop()
        if not rgs:
            st.warning("No resource groups in this subscription.")
            st.stop()
        rg_name = st.selectbox("Resource group", options=rgs, index=0)
        try:
            res_rows = list_rg_resources(_credential=credential, subscription_id=subscription_id, resource_group=rg_name)
            if res_rows:
                st.caption(f"Resources in '{rg_name}' ({len(res_rows)} found)")
                st.dataframe(res_rows, use_container_width=True, hide_index=True)
            else:
                st.info("No resources found in this resource group.")
        except Exception as e:
            st.warning(f"Could not list resources in '{rg_name}': {e}")

    # Data factory selection
    with st.container(border=True):
        st.subheader("4. Select data factories")
        try:
            factories = list_data_factories(_credential=credential, subscription_id=subscription_id, resource_group=rg_name)
        except Exception as e:
            st.error(f"Failed to list data factories: {e}")
            st.stop()
        if not factories:
            st.warning("No data factories in this resource group.")
            st.stop()
        select_all = st.checkbox("Select all factories", value=True)
        selected_factories = factories if select_all else st.multiselect("Factories", options=factories, default=factories[:1])

    # Fetch components
    with st.container(border=True):
        st.subheader("5. Components per factory")
        run = st.button("Fetch components", type="primary")
        if run:
            rows: List[Dict[str, str]] = []
            try:
                targets = factories if (select_all or not selected_factories) else selected_factories
                for fac in targets:
                    rows.extend(fetch_activity_rows_for_factory(credential, subscription_id, rg_name, fac))
            except Exception as e:
                st.error(f"Failed to fetch components: {e}")
                st.stop()

            if rows:
                st.success(f"Found {len(rows)} activity entries.")
                st.dataframe(rows, use_container_width=True, hide_index=True)
                # Download as CSV
                import io, csv

                buf = io.StringIO()
                writer = csv.DictWriter(buf, fieldnames=["Factory", "PipelineName", "ActivityName", "ActivityType", "Migratable", "Category", "Activated", "Description", "SourceQuery"])
                writer.writeheader()
                writer.writerows(rows)
                st.download_button("Download CSV", data=buf.getvalue(), file_name="adf_activities.csv", mime="text/csv")
                # Keep rows for scoring
                st.session_state["activity_rows"] = rows
            else:
                st.info("No components found.")

    # Migration scoring based on four parameters
    with st.container(border=True):
        st.subheader("5a. Migration scoring (Fabric readiness)")
        if "activity_rows" not in st.session_state:
            st.info("Fetch components first to compute scores.")
        else:
            calc = st.button("Compute migration scores")
            if calc:
                import io, csv
                rows = st.session_state.get("activity_rows", [])
                # Group by (Factory, PipelineName)
                from collections import defaultdict
                grouped: Dict[Tuple[str, str], List[Dict[str, str]]] = defaultdict(list)
                for r in rows:
                    grouped[(r.get("Factory", ""), r.get("PipelineName", ""))].append(r)

                # Preload linked service types per factory
                ls_types_by_factory: Dict[str, List[str]] = {}
                targets = factories if (select_all or not selected_factories) else selected_factories
                for fac in targets:
                    try:
                        ls_rows = list_linked_services_for_factory(credential, subscription_id, rg_name, fac)
                        ls_types_by_factory[fac] = [row.get("LinkedServiceType", "") for row in ls_rows]
                    except Exception:
                        ls_types_by_factory[fac] = []

                score_rows: List[Dict[str, Any]] = []
                for (fac, pipe), items in grouped.items():
                    total_acts = len(items)
                    non_migratable = sum(1 for it in items if (it.get("Migratable") or "").lower() == "no")
                    # Control activity count by type name
                    control_acts = 0
                    for it in items:
                        t = it.get("ActivityType")
                        nt = _normalize_type(t)
                        if nt in CONTROL_ACTIVITY_TYPES:
                            control_acts += 1

                    parity_score = _score_component_parity(total_acts, non_migratable)
                    non_mig_score = _score_non_migratable(non_migratable)
                    connectivity_score = _score_connectivity(ls_types_by_factory.get(fac, []))
                    orchestration_score = _score_orchestration(total_acts, control_acts)
                    total = parity_score + non_mig_score + connectivity_score + orchestration_score
                    # Banding with escalation if any score == 3
                    if parity_score == 3 or non_mig_score == 3 or connectivity_score == 3 or orchestration_score == 3:
                        band = "Hard"
                    elif total <= 4:
                        band = "Easy"
                    elif total <= 8:
                        band = "Medium"
                    else:
                        band = "Hard"

                    score_rows.append({
                        "Factory": fac,
                        "Pipeline": pipe,
                        "Component parity": parity_score,
                        "Non-migratable count": non_mig_score,
                        "Connectivity": connectivity_score,
                        "Orchestration": orchestration_score,
                        "Total": total,
                        "Band": band,
                        "Activities": total_acts,
                        "Non-migratable": non_migratable,
                    })

                if score_rows:
                    st.dataframe(score_rows, use_container_width=True, hide_index=True)
                    buf = io.StringIO()
                    writer = csv.DictWriter(buf, fieldnames=[
                        "Factory","Pipeline","Component parity","Non-migratable count","Connectivity","Orchestration","Total","Band","Activities","Non-migratable"
                    ])
                    writer.writeheader()
                    writer.writerows(score_rows)
                    st.download_button("Download scores CSV", data=buf.getvalue(), file_name="adf_migration_scores.csv", mime="text/csv")
                else:
                    st.info("No pipelines to score.")

    with st.container(border=True):
        st.subheader("6. Datasets per factory")
        run_ds = st.button("Fetch datasets")
        if run_ds:
            ds_rows: List[Dict[str, str]] = []
            try:
                targets = factories if (select_all or not selected_factories) else selected_factories
                for fac in targets:
                    ds_rows.extend(list_datasets_for_factory(credential, subscription_id, rg_name, fac))
            except Exception as e:
                st.error(f"Failed to fetch datasets: {e}")
                st.stop()
            if ds_rows:
                st.success(f"Found {len(ds_rows)} datasets.")
                st.dataframe(ds_rows, use_container_width=True, hide_index=True)
                import io, csv
                buf_ds = io.StringIO()
                writer_ds = csv.DictWriter(buf_ds, fieldnames=["Factory", "DatasetName", "DatasetType", "LinkedService"])
                writer_ds.writeheader()
                writer_ds.writerows(ds_rows)
                st.download_button("Download Datasets CSV", data=buf_ds.getvalue(), file_name="adf_datasets.csv", mime="text/csv")
            else:
                st.info("No datasets found.")

    with st.container(border=True):
        st.subheader("7. Linked services per factory")
        run_ls = st.button("Fetch linked services")
        if run_ls:
            ls_rows: List[Dict[str, str]] = []
            try:
                targets = factories if (select_all or not selected_factories) else selected_factories
                for fac in targets:
                    ls_rows.extend(list_linked_services_for_factory(credential, subscription_id, rg_name, fac))
            except Exception as e:
                st.error(f"Failed to fetch linked services: {e}")
                st.stop()
            if ls_rows:
                st.success(f"Found {len(ls_rows)} linked services.")
                st.dataframe(ls_rows, use_container_width=True, hide_index=True)
                import io, csv
                buf2 = io.StringIO()
                writer2 = csv.DictWriter(buf2, fieldnames=["Factory", "LinkedService", "LinkedServiceType"]) 
                writer2.writeheader()
                writer2.writerows(ls_rows)
                st.download_button("Download Linked Services CSV", data=buf2.getvalue(), file_name="adf_linked_services.csv", mime="text/csv")
            else:
                st.info("No linked services found.")

    # Storage browser
    with st.container(border=True):
        st.subheader("8. Storage browser (resource group)")
        try:
            sa_names = list_storage_accounts(_credential=credential, subscription_id=subscription_id, resource_group=rg_name)
        except Exception as e:
            st.error(f"Failed to list storage accounts: {e}")
            sa_names = []
        if not sa_names:
            st.info("No storage accounts found in this resource group.")
        else:
            sa = st.selectbox("Storage account", options=sa_names, index=0)
            try:
                containers = list_blob_containers(_credential=credential, subscription_id=subscription_id, resource_group=rg_name, account_name=sa)
            except Exception as e:
                st.error(f"Failed to list containers for {sa}: {e}")
                containers = []
            if not containers:
                st.info("No containers found or access denied.")
            else:
                cont = st.selectbox("Container", options=containers, index=0)
                show_files = st.checkbox("Show files under each folder (may be slow)", value=False)
                show_diag = st.checkbox("Show storage diagnostics", value=False)
                # Detect HNS and branch
                hns = is_hns_enabled(_credential=credential, subscription_id=subscription_id, resource_group=rg_name, account_name=sa)
                if show_diag:
                    with st.expander("Diagnostics", expanded=False):
                        st.caption(f"Diagnostics â†’ HNS enabled: {'Yes' if hns else 'No'}")
                if st.button("List folders"):
                    folder_rows: List[Dict[str, str]] = []
                    folder_names: List[str] = []
                    diag_errors: List[str] = []
                    adls_samples: List[Dict[str, Any]] = []
                    blob_samples: List[Dict[str, Any]] = []
                    try:
                        if hns:
                            folder_rows = list_adls_top_level_directories(_credential=credential, account_name=sa, filesystem=cont)
                            folder_names = [row.get("Folder", "") for row in folder_rows if row.get("Folder")]
                        else:
                            folder_names = list_top_level_folders(_credential=credential, account_name=sa, container_name=cont)
                            folder_rows = [{"Folder": name, "LastModified": ""} for name in folder_names]
                    except Exception as exc:
                        diag_errors.append(f"Folder listing error ({'ADLS' if hns else 'Blob'}): {exc}")

                    if folder_names:
                        st.success(f"Found {len(folder_names)} folders in {cont}.")
                        st.dataframe(folder_rows, use_container_width=True, hide_index=True)
                        if show_files:
                            for f in folder_names:
                                with st.expander(f"Files in {f}"):
                                    try:
                                        if hns:
                                            files = list_adls_files_in_directory(_credential=credential, account_name=sa, filesystem=cont, directory=f)
                                        else:
                                            files = list_files_in_folder(_credential=credential, account_name=sa, container_name=cont, folder=f)
                                        if files:
                                            st.write("\n".join(files))
                                        else:
                                            st.caption("No files or access denied.")
                                    except Exception as exc:
                                        st.error(f"Failed to list files for {f}: {exc}")
                    else:
                        st.info("No folders found or access denied.")

                    if show_diag:
                        try:
                            if hns:
                                adls_samples = sample_adls_paths(_credential=credential, account_name=sa, filesystem=cont, limit=10)
                            else:
                                blob_samples = sample_blob_paths(_credential=credential, account_name=sa, container_name=cont, limit=10)
                        except Exception as exc:
                            diag_errors.append(f"Sample listing error ({'ADLS' if hns else 'Blob'}): {exc}")

                        if diag_errors:
                            st.error("Diagnostics errors:\n" + "\n".join(diag_errors))
                        else:
                            st.info("Diagnostics: no errors recorded during listing attempts.")

                        if adls_samples:
                            st.json(adls_samples)
                        if blob_samples:
                            st.caption("Sample Blob paths (first 10):")
                            st.json(blob_samples)


    # Lineage view
    with st.container(border=True):
        st.subheader("9. Lineage view")
        run_lineage = st.button("Build lineage graph")
        if run_lineage:
            try:
                targets = factories if (select_all or not selected_factories) else selected_factories
                dot = build_lineage_dot(credential, subscription_id, rg_name, targets)
                st.graphviz_chart(dot, use_container_width=True)
                import io
                st.download_button("Download DOT", data=dot, file_name="adf_lineage.dot", mime="text/vnd.graphviz")
            except Exception as e:
                st.error(f"Failed to build lineage: {e}")

if __name__ == "__main__":
    main()




